{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlo4Ctmtm4p"
      },
      "source": [
        "# Transfer Learning Assignment\n",
        "반갑습니다 여러분. 과제를 맡은 김준호입니다. TL 세션에서 다룬 Fine-tuning과 Domain Adaptation을 직접 구현해 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlAlAeRtm4s"
      },
      "source": [
        "Office-31은 여러 실습에서 자주 등장하는 이미지 데이터셋입니다. Amazon, Webcam, DSLR 세 개의 도메인으로 구성되어 있는데요. \n",
        "\n",
        "총 31개 클래스 (키보드, 마우스, 모니터 등)의 사무용품 이미지가 있고,각 도메인마다 같은 클래스가 포함되어 있지만 도메인마다 이미지 특성은 다릅니다. \n",
        "\n",
        "그래서 이런 transfer learning 실습에 적합하다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6R098oQTS_TC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n",
        "torch.cuda.set_device(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ut17hwUDq-"
      },
      "source": [
        "amazon을 source로, webcam을 target data로 이용해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j4e--IIRU68M"
      },
      "outputs": [],
      "source": [
        "data_folder = '../OFFICE31'\n",
        "batch_size = 32\n",
        "n_class = 31\n",
        "domain_src, domain_tar = 'amazon', 'webcam'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6e2YaPtm4u"
      },
      "source": [
        "source와 target domain에 대한 DataLoader를 생성하고 load해 줍시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6JtN_bK9VFcM"
      },
      "outputs": [],
      "source": [
        "def load_data(root_path, domain, batch_size, phase):\n",
        "    transform_dict = {\n",
        "        'src': transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ]),\n",
        "        'tar': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ])}\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path,domain), transform=transform_dict[phase])\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../OFFICE31/amazon\n"
          ]
        }
      ],
      "source": [
        "print(os.path.join(data_folder, domain_src))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Jf_Gw2HRVJM_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source data number: 2817\n",
            "Target data number: 795\n"
          ]
        }
      ],
      "source": [
        "src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n",
        "tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n",
        "print(f'Source data number: {len(src_loader.dataset)}')\n",
        "print(f'Target data number: {len(tar_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caKMn438tm4v"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-trained ResNet50을 기반으로 한 간단한 TransferModel을 정의합니다. 여기서 모델 구조는 유지하되 마지막 fc layer만 새롭게 학습되도록 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OXAjmY7pVK8t"
      },
      "outputs": [],
      "source": [
        "class TransferModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                base_model : str = 'resnet50',\n",
        "                pretrain : bool = True,\n",
        "                n_class : int = 31):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.pretrain = pretrain\n",
        "        self.n_class = n_class\n",
        "        if self.base_model == 'resnet50':\n",
        "            self.model = models.resnet50(pretrained=self.pretrain)\n",
        "            # TODO: pre-trained model 불러오기\n",
        "            n_features = self.model.fc.in_features\n",
        "            fc = nn.Linear(n_features, self.n_class)\n",
        "            self.model.fc = fc\n",
        "            # TODO: 새로운 fc layer를 정의\n",
        "            # TODO: 모델의 fc layer를 새로운 fc로 교체\n",
        "        else:\n",
        "            # Use other models you like, such as vgg or alexnet\n",
        "            pass\n",
        "        self.model.fc.weight.data.normal_(0, 0.005)\n",
        "        self.model.fc.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqT-0jRtm4w"
      },
      "source": [
        "모델이 정상적으로 작동하는지 random tensor로 테스트해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LewRmYIvXEIo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/detectron2/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/detectron2/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1814,  0.1709,  0.0338,  0.1379,  0.1117,  0.0951,  0.0388, -0.0496,\n",
            "          0.0607,  0.2215,  0.0925,  0.1570,  0.0296,  0.1868,  0.1846, -0.1066,\n",
            "          0.1243,  0.0592,  0.0817,  0.2285, -0.0429, -0.1672,  0.1379,  0.1832,\n",
            "          0.1125,  0.0691, -0.0354,  0.0687,  0.0527,  0.1365,  0.0467]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 31])\n"
          ]
        }
      ],
      "source": [
        "model = TransferModel().cuda()\n",
        "RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n",
        "output = model(RAND_TENSOR)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpX-fuiXtm4w"
      },
      "source": [
        "## Finetune ResNet-50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNeAiPatm4w"
      },
      "source": [
        "Office-31 dataset은 validation set이 따로 없으므로, validation set으로 target domain을 이용해 줍시다.\n",
        "\n",
        "fine-tuning을 위한 학습 및 평가 함수를 정의합니다.\n",
        "학습은 source domain에서 수행하고 target domain에서 검증합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h74gKIVqtm4w"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'src': src_loader,\n",
        "               'val': tar_loader,\n",
        "               'tar': tar_loader}\n",
        "n_epoch = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stop = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-fz_FlAIXsTF"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def finetune(model, dataloaders, optimizer):\n",
        "    since = time.time()\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    for epoch in tqdm(range(0, n_epoch)):\n",
        "        stop += 1\n",
        "        # You can uncomment this line for scheduling learning rate\n",
        "        # lr_schedule(optimizer, epoch)\n",
        "        for phase in ['src', 'val', 'tar']:\n",
        "            if phase == 'src':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            total_loss, correct = 0, 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'src'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                preds = torch.max(outputs, 1)[1]\n",
        "                if phase == 'src':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "            epoch_loss = total_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / len(dataloaders[phase].dataset)\n",
        "            print(f'Epoch: [{epoch:02d}/{n_epoch:02d}]---{phase}, loss: {epoch_loss:.6f}, acc: {epoch_acc:.4f}')\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                stop = 0\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), 'model.pkl')\n",
        "        if stop >= early_stop:\n",
        "            break\n",
        "        print()\n",
        "   \n",
        "    time_pass = time.time() - since\n",
        "    print(f'Training complete in {time_pass // 60:.0f}m {time_pass % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGUZGrm7tm4x"
      },
      "source": [
        "이제 학습 파라미터들과 optimizer를 정의합니다.\n",
        "간단하게 SGD optimizer를 사용하고 fc layer의 학습률을 다른 layer보다 10배 크게 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HGVIu0JXZaGT"
      },
      "outputs": [],
      "source": [
        "param_group = []\n",
        "learning_rate = 0.0001\n",
        "momentum = 5e-4\n",
        "for k, v in model.named_parameters():\n",
        "    if not k.__contains__('fc'):\n",
        "        param_group += [{'params': v, 'lr': learning_rate}]\n",
        "    else:\n",
        "        param_group += [{'params': v, 'lr': learning_rate * 10}]\n",
        "optimizer = torch.optim.SGD(param_group, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7zGR_PFtm4y"
      },
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uKKrah-AZsHt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [00/100]---src, loss: 3.381703, acc: 0.1040\n",
            "Epoch: [00/100]---val, loss: 3.204651, acc: 0.0252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:09<15:03,  9.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [00/100]---tar, loss: 3.204651, acc: 0.0252\n",
            "\n",
            "Epoch: [01/100]---src, loss: 3.250616, acc: 0.2368\n",
            "Epoch: [01/100]---val, loss: 3.081978, acc: 0.0843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:18<14:45,  9.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [01/100]---tar, loss: 3.081978, acc: 0.0843\n",
            "\n",
            "Epoch: [02/100]---src, loss: 3.126556, acc: 0.4189\n",
            "Epoch: [02/100]---val, loss: 2.962365, acc: 0.2478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/100 [00:27<14:42,  9.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [02/100]---tar, loss: 2.962365, acc: 0.2478\n",
            "\n",
            "Epoch: [03/100]---src, loss: 3.003831, acc: 0.5119\n",
            "Epoch: [03/100]---val, loss: 2.863927, acc: 0.3799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [00:36<14:26,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [03/100]---tar, loss: 2.863927, acc: 0.3799\n",
            "\n",
            "Epoch: [04/100]---src, loss: 2.877881, acc: 0.5871\n",
            "Epoch: [04/100]---val, loss: 2.735294, acc: 0.5358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5/100 [00:45<14:17,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [04/100]---tar, loss: 2.735294, acc: 0.5358\n",
            "\n",
            "Epoch: [05/100]---src, loss: 2.756365, acc: 0.6081\n",
            "Epoch: [05/100]---val, loss: 2.621636, acc: 0.5925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:54<14:06,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [05/100]---tar, loss: 2.621636, acc: 0.5925\n",
            "\n",
            "Epoch: [06/100]---src, loss: 2.633124, acc: 0.6368\n",
            "Epoch: [06/100]---val, loss: 2.492890, acc: 0.6063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7/100 [01:03<13:57,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [06/100]---tar, loss: 2.492890, acc: 0.6063\n",
            "\n",
            "Epoch: [07/100]---src, loss: 2.520446, acc: 0.6464\n",
            "Epoch: [07/100]---val, loss: 2.399228, acc: 0.6063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 8/100 [01:12<13:45,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [07/100]---tar, loss: 2.399228, acc: 0.6063\n",
            "\n",
            "Epoch: [08/100]---src, loss: 2.416345, acc: 0.6429\n",
            "Epoch: [08/100]---val, loss: 2.295598, acc: 0.5937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 9/100 [01:21<13:34,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [08/100]---tar, loss: 2.295598, acc: 0.5937\n",
            "\n",
            "Epoch: [09/100]---src, loss: 2.311779, acc: 0.6571\n",
            "Epoch: [09/100]---val, loss: 2.185512, acc: 0.6453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10/100 [01:30<13:30,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [09/100]---tar, loss: 2.185512, acc: 0.6453\n",
            "\n",
            "Epoch: [10/100]---src, loss: 2.228729, acc: 0.6564\n",
            "Epoch: [10/100]---val, loss: 2.170633, acc: 0.6314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [01:39<13:20,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [10/100]---tar, loss: 2.170633, acc: 0.6314\n",
            "\n",
            "Epoch: [11/100]---src, loss: 2.110586, acc: 0.6880\n",
            "Epoch: [11/100]---val, loss: 2.072694, acc: 0.6591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 12/100 [01:48<13:12,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [11/100]---tar, loss: 2.072694, acc: 0.6591\n",
            "\n",
            "Epoch: [12/100]---src, loss: 2.038191, acc: 0.6805\n",
            "Epoch: [12/100]---val, loss: 1.950767, acc: 0.6491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 13/100 [01:57<13:01,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [12/100]---tar, loss: 1.950767, acc: 0.6491\n",
            "\n",
            "Epoch: [13/100]---src, loss: 1.945398, acc: 0.6880\n",
            "Epoch: [13/100]---val, loss: 1.902596, acc: 0.6679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 14/100 [02:06<12:54,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [13/100]---tar, loss: 1.902596, acc: 0.6679\n",
            "\n",
            "Epoch: [14/100]---src, loss: 1.877682, acc: 0.7011\n",
            "Epoch: [14/100]---val, loss: 1.819073, acc: 0.7019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 15/100 [02:15<12:50,  9.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [14/100]---tar, loss: 1.819073, acc: 0.7019\n",
            "\n",
            "Epoch: [15/100]---src, loss: 1.813515, acc: 0.7032\n",
            "Epoch: [15/100]---val, loss: 1.722380, acc: 0.6994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16/100 [02:24<12:38,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [15/100]---tar, loss: 1.722380, acc: 0.6994\n",
            "\n",
            "Epoch: [16/100]---src, loss: 1.742982, acc: 0.6976\n",
            "Epoch: [16/100]---val, loss: 1.689775, acc: 0.6994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 17/100 [02:33<12:26,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [16/100]---tar, loss: 1.689775, acc: 0.6994\n",
            "\n",
            "Epoch: [17/100]---src, loss: 1.683765, acc: 0.7061\n",
            "Epoch: [17/100]---val, loss: 1.657242, acc: 0.6805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 18/100 [02:42<12:17,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [17/100]---tar, loss: 1.657242, acc: 0.6805\n",
            "\n",
            "Epoch: [18/100]---src, loss: 1.628317, acc: 0.7039\n",
            "Epoch: [18/100]---val, loss: 1.589518, acc: 0.7006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 19/100 [02:51<12:07,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [18/100]---tar, loss: 1.589518, acc: 0.7006\n",
            "\n",
            "Epoch: [19/100]---src, loss: 1.588949, acc: 0.7174\n",
            "Epoch: [19/100]---val, loss: 1.551071, acc: 0.6956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20/100 [03:00<11:59,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [19/100]---tar, loss: 1.551071, acc: 0.6956\n",
            "\n",
            "Epoch: [20/100]---src, loss: 1.528567, acc: 0.7235\n",
            "Epoch: [20/100]---val, loss: 1.488074, acc: 0.7006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21/100 [03:09<11:49,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [20/100]---tar, loss: 1.488074, acc: 0.7006\n",
            "\n",
            "Epoch: [21/100]---src, loss: 1.473435, acc: 0.7249\n",
            "Epoch: [21/100]---val, loss: 1.436496, acc: 0.7195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 22/100 [03:18<11:41,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [21/100]---tar, loss: 1.436496, acc: 0.7195\n",
            "\n",
            "Epoch: [22/100]---src, loss: 1.447155, acc: 0.7281\n",
            "Epoch: [22/100]---val, loss: 1.411091, acc: 0.7107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 23/100 [03:27<11:35,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [22/100]---tar, loss: 1.411091, acc: 0.7107\n",
            "\n",
            "Epoch: [23/100]---src, loss: 1.416882, acc: 0.7259\n",
            "Epoch: [23/100]---val, loss: 1.368797, acc: 0.7119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 24/100 [03:36<11:25,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [23/100]---tar, loss: 1.368797, acc: 0.7119\n",
            "\n",
            "Epoch: [24/100]---src, loss: 1.369257, acc: 0.7384\n",
            "Epoch: [24/100]---val, loss: 1.355132, acc: 0.7145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25/100 [03:45<11:17,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [24/100]---tar, loss: 1.355132, acc: 0.7145\n",
            "\n",
            "Epoch: [25/100]---src, loss: 1.366332, acc: 0.7345\n",
            "Epoch: [25/100]---val, loss: 1.345599, acc: 0.7145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 26/100 [03:54<11:08,  9.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [25/100]---tar, loss: 1.345599, acc: 0.7145\n",
            "\n",
            "Epoch: [26/100]---src, loss: 1.302545, acc: 0.7476\n",
            "Epoch: [26/100]---val, loss: 1.325004, acc: 0.6855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 27/100 [04:03<10:58,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [26/100]---tar, loss: 1.325004, acc: 0.6855\n",
            "\n",
            "Epoch: [27/100]---src, loss: 1.288349, acc: 0.7394\n",
            "Epoch: [27/100]---val, loss: 1.302233, acc: 0.7069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 28/100 [04:12<10:48,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [27/100]---tar, loss: 1.302233, acc: 0.7069\n",
            "\n",
            "Epoch: [28/100]---src, loss: 1.271677, acc: 0.7433\n",
            "Epoch: [28/100]---val, loss: 1.274880, acc: 0.7082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 29/100 [04:21<10:37,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [28/100]---tar, loss: 1.274880, acc: 0.7082\n",
            "\n",
            "Epoch: [29/100]---src, loss: 1.234554, acc: 0.7451\n",
            "Epoch: [29/100]---val, loss: 1.253335, acc: 0.7308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 30/100 [04:30<10:31,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [29/100]---tar, loss: 1.253335, acc: 0.7308\n",
            "\n",
            "Epoch: [30/100]---src, loss: 1.207392, acc: 0.7504\n",
            "Epoch: [30/100]---val, loss: 1.227240, acc: 0.7145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 31/100 [04:39<10:22,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [30/100]---tar, loss: 1.227240, acc: 0.7145\n",
            "\n",
            "Epoch: [31/100]---src, loss: 1.187616, acc: 0.7536\n",
            "Epoch: [31/100]---val, loss: 1.207308, acc: 0.7182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 32/100 [04:48<10:14,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [31/100]---tar, loss: 1.207308, acc: 0.7182\n",
            "\n",
            "Epoch: [32/100]---src, loss: 1.165484, acc: 0.7618\n",
            "Epoch: [32/100]---val, loss: 1.203066, acc: 0.7006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 33/100 [04:57<10:03,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [32/100]---tar, loss: 1.203066, acc: 0.7006\n",
            "\n",
            "Epoch: [33/100]---src, loss: 1.148647, acc: 0.7512\n",
            "Epoch: [33/100]---val, loss: 1.161803, acc: 0.7258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 34/100 [05:06<09:55,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [33/100]---tar, loss: 1.161803, acc: 0.7258\n",
            "\n",
            "Epoch: [34/100]---src, loss: 1.127523, acc: 0.7604\n",
            "Epoch: [34/100]---val, loss: 1.150788, acc: 0.7082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 35/100 [05:15<09:44,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [34/100]---tar, loss: 1.150788, acc: 0.7082\n",
            "\n",
            "Epoch: [35/100]---src, loss: 1.122064, acc: 0.7600\n",
            "Epoch: [35/100]---val, loss: 1.155091, acc: 0.7132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 36/100 [05:24<09:35,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [35/100]---tar, loss: 1.155091, acc: 0.7132\n",
            "\n",
            "Epoch: [36/100]---src, loss: 1.092128, acc: 0.7618\n",
            "Epoch: [36/100]---val, loss: 1.098797, acc: 0.7308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 37/100 [05:33<09:26,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [36/100]---tar, loss: 1.098797, acc: 0.7308\n",
            "\n",
            "Epoch: [37/100]---src, loss: 1.085734, acc: 0.7554\n",
            "Epoch: [37/100]---val, loss: 1.145417, acc: 0.7145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 38/100 [05:42<09:17,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [37/100]---tar, loss: 1.145417, acc: 0.7145\n",
            "\n",
            "Epoch: [38/100]---src, loss: 1.062392, acc: 0.7664\n",
            "Epoch: [38/100]---val, loss: 1.093091, acc: 0.7233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 39/100 [05:51<09:07,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [38/100]---tar, loss: 1.093091, acc: 0.7233\n",
            "\n",
            "Epoch: [39/100]---src, loss: 1.056961, acc: 0.7664\n",
            "Epoch: [39/100]---val, loss: 1.058087, acc: 0.7358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 40/100 [06:00<09:01,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [39/100]---tar, loss: 1.058087, acc: 0.7358\n",
            "\n",
            "Epoch: [40/100]---src, loss: 1.040437, acc: 0.7597\n",
            "Epoch: [40/100]---val, loss: 1.102717, acc: 0.7119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [06:09<08:50,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [40/100]---tar, loss: 1.102717, acc: 0.7119\n",
            "\n",
            "Epoch: [41/100]---src, loss: 1.027981, acc: 0.7742\n",
            "Epoch: [41/100]---val, loss: 1.060499, acc: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 42/100 [06:18<08:43,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [41/100]---tar, loss: 1.060499, acc: 0.7384\n",
            "\n",
            "Epoch: [42/100]---src, loss: 1.009102, acc: 0.7714\n",
            "Epoch: [42/100]---val, loss: 1.027042, acc: 0.7333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 43/100 [06:27<08:33,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [42/100]---tar, loss: 1.027042, acc: 0.7333\n",
            "\n",
            "Epoch: [43/100]---src, loss: 1.006009, acc: 0.7629\n",
            "Epoch: [43/100]---val, loss: 1.072955, acc: 0.7157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [06:36<08:26,  9.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [43/100]---tar, loss: 1.072955, acc: 0.7157\n",
            "\n",
            "Epoch: [44/100]---src, loss: 0.984499, acc: 0.7760\n",
            "Epoch: [44/100]---val, loss: 1.009411, acc: 0.7321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 45/100 [06:45<08:16,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [44/100]---tar, loss: 1.009411, acc: 0.7321\n",
            "\n",
            "Epoch: [45/100]---src, loss: 0.995841, acc: 0.7753\n",
            "Epoch: [45/100]---val, loss: 1.090577, acc: 0.7208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 46/100 [06:54<08:04,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [45/100]---tar, loss: 1.090577, acc: 0.7208\n",
            "\n",
            "Epoch: [46/100]---src, loss: 0.965388, acc: 0.7753\n",
            "Epoch: [46/100]---val, loss: 1.016375, acc: 0.7371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 47/100 [07:03<07:56,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [46/100]---tar, loss: 1.016375, acc: 0.7371\n",
            "\n",
            "Epoch: [47/100]---src, loss: 0.969776, acc: 0.7717\n",
            "Epoch: [47/100]---val, loss: 1.028213, acc: 0.7031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 48/100 [07:12<07:49,  9.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [47/100]---tar, loss: 1.028213, acc: 0.7031\n",
            "\n",
            "Epoch: [48/100]---src, loss: 0.939282, acc: 0.7806\n",
            "Epoch: [48/100]---val, loss: 0.987386, acc: 0.7321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 49/100 [07:21<07:37,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [48/100]---tar, loss: 0.987386, acc: 0.7321\n",
            "\n",
            "Epoch: [49/100]---src, loss: 0.924547, acc: 0.7792\n",
            "Epoch: [49/100]---val, loss: 0.999949, acc: 0.7208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 50/100 [07:30<07:27,  8.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [49/100]---tar, loss: 0.999949, acc: 0.7208\n",
            "\n",
            "Epoch: [50/100]---src, loss: 0.920158, acc: 0.7820\n",
            "Epoch: [50/100]---val, loss: 0.974876, acc: 0.7170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 51/100 [07:39<07:18,  8.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [50/100]---tar, loss: 0.974876, acc: 0.7170\n",
            "\n",
            "Epoch: [51/100]---src, loss: 0.915946, acc: 0.7842\n",
            "Epoch: [51/100]---val, loss: 0.958878, acc: 0.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 52/100 [07:48<07:10,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [51/100]---tar, loss: 0.958878, acc: 0.7296\n",
            "\n",
            "Epoch: [52/100]---src, loss: 0.913862, acc: 0.7838\n",
            "Epoch: [52/100]---val, loss: 0.980287, acc: 0.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 53/100 [07:57<07:00,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [52/100]---tar, loss: 0.980287, acc: 0.7296\n",
            "\n",
            "Epoch: [53/100]---src, loss: 0.911181, acc: 0.7817\n",
            "Epoch: [53/100]---val, loss: 0.952533, acc: 0.7421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 54/100 [08:06<06:54,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [53/100]---tar, loss: 0.952533, acc: 0.7421\n",
            "\n",
            "Epoch: [54/100]---src, loss: 0.906056, acc: 0.7767\n",
            "Epoch: [54/100]---val, loss: 0.964771, acc: 0.7270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [08:15<06:44,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [54/100]---tar, loss: 0.964771, acc: 0.7270\n",
            "\n",
            "Epoch: [55/100]---src, loss: 0.895442, acc: 0.7895\n",
            "Epoch: [55/100]---val, loss: 0.945152, acc: 0.7358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 56/100 [08:24<06:35,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [55/100]---tar, loss: 0.945152, acc: 0.7358\n",
            "\n",
            "Epoch: [56/100]---src, loss: 0.871945, acc: 0.7934\n",
            "Epoch: [56/100]---val, loss: 0.919610, acc: 0.7509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 57/100 [08:33<06:27,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [56/100]---tar, loss: 0.919610, acc: 0.7509\n",
            "\n",
            "Epoch: [57/100]---src, loss: 0.892523, acc: 0.7842\n",
            "Epoch: [57/100]---val, loss: 0.918588, acc: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 58/100 [08:42<06:18,  9.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [57/100]---tar, loss: 0.918588, acc: 0.7384\n",
            "\n",
            "Epoch: [58/100]---src, loss: 0.871297, acc: 0.7877\n",
            "Epoch: [58/100]---val, loss: 0.909696, acc: 0.7371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 59/100 [08:51<06:09,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [58/100]---tar, loss: 0.909696, acc: 0.7371\n",
            "\n",
            "Epoch: [59/100]---src, loss: 0.861764, acc: 0.7888\n",
            "Epoch: [59/100]---val, loss: 0.939587, acc: 0.7195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [09:00<05:59,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [59/100]---tar, loss: 0.939587, acc: 0.7195\n",
            "\n",
            "Epoch: [60/100]---src, loss: 0.855246, acc: 0.7909\n",
            "Epoch: [60/100]---val, loss: 0.905055, acc: 0.7346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [09:09<05:50,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [60/100]---tar, loss: 0.905055, acc: 0.7346\n",
            "\n",
            "Epoch: [61/100]---src, loss: 0.846241, acc: 0.7909\n",
            "Epoch: [61/100]---val, loss: 0.887712, acc: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 62/100 [09:18<05:41,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [61/100]---tar, loss: 0.887712, acc: 0.7384\n",
            "\n",
            "Epoch: [62/100]---src, loss: 0.831492, acc: 0.8001\n",
            "Epoch: [62/100]---val, loss: 0.936709, acc: 0.7258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 63/100 [09:27<05:31,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [62/100]---tar, loss: 0.936709, acc: 0.7258\n",
            "\n",
            "Epoch: [63/100]---src, loss: 0.846901, acc: 0.7938\n",
            "Epoch: [63/100]---val, loss: 0.916481, acc: 0.7245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 64/100 [09:36<05:23,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [63/100]---tar, loss: 0.916481, acc: 0.7245\n",
            "\n",
            "Epoch: [64/100]---src, loss: 0.843342, acc: 0.8001\n",
            "Epoch: [64/100]---val, loss: 0.908445, acc: 0.7409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [09:45<05:14,  9.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [64/100]---tar, loss: 0.908445, acc: 0.7409\n",
            "\n",
            "Epoch: [65/100]---src, loss: 0.817815, acc: 0.8030\n",
            "Epoch: [65/100]---val, loss: 0.910394, acc: 0.7157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 66/100 [09:54<05:05,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [65/100]---tar, loss: 0.910394, acc: 0.7157\n",
            "\n",
            "Epoch: [66/100]---src, loss: 0.814973, acc: 0.8040\n",
            "Epoch: [66/100]---val, loss: 0.875569, acc: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 67/100 [10:03<04:56,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [66/100]---tar, loss: 0.875569, acc: 0.7384\n",
            "\n",
            "Epoch: [67/100]---src, loss: 0.801670, acc: 0.8009\n",
            "Epoch: [67/100]---val, loss: 0.891216, acc: 0.7283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 68/100 [10:11<04:46,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [67/100]---tar, loss: 0.891216, acc: 0.7283\n",
            "\n",
            "Epoch: [68/100]---src, loss: 0.778359, acc: 0.8072\n",
            "Epoch: [68/100]---val, loss: 0.896764, acc: 0.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 69/100 [10:20<04:38,  8.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [68/100]---tar, loss: 0.896764, acc: 0.7296\n",
            "\n",
            "Epoch: [69/100]---src, loss: 0.790640, acc: 0.7941\n",
            "Epoch: [69/100]---val, loss: 0.894162, acc: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 70/100 [10:29<04:28,  8.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [69/100]---tar, loss: 0.894162, acc: 0.7384\n",
            "\n",
            "Epoch: [70/100]---src, loss: 0.818671, acc: 0.7923\n",
            "Epoch: [70/100]---val, loss: 0.932846, acc: 0.7208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 71/100 [10:38<04:18,  8.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [70/100]---tar, loss: 0.932846, acc: 0.7208\n",
            "\n",
            "Epoch: [71/100]---src, loss: 0.785742, acc: 0.7973\n",
            "Epoch: [71/100]---val, loss: 0.859789, acc: 0.7421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 72/100 [10:47<04:08,  8.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [71/100]---tar, loss: 0.859789, acc: 0.7421\n",
            "\n",
            "Epoch: [72/100]---src, loss: 0.776938, acc: 0.8062\n",
            "Epoch: [72/100]---val, loss: 0.847285, acc: 0.7447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [10:56<04:01,  8.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [72/100]---tar, loss: 0.847285, acc: 0.7447\n",
            "\n",
            "Epoch: [73/100]---src, loss: 0.801786, acc: 0.8005\n",
            "Epoch: [73/100]---val, loss: 0.871232, acc: 0.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 74/100 [11:05<03:52,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [73/100]---tar, loss: 0.871232, acc: 0.7296\n",
            "\n",
            "Epoch: [74/100]---src, loss: 0.756912, acc: 0.8122\n",
            "Epoch: [74/100]---val, loss: 0.846325, acc: 0.7396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [11:14<03:43,  8.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [74/100]---tar, loss: 0.846325, acc: 0.7396\n",
            "\n",
            "Epoch: [75/100]---src, loss: 0.777926, acc: 0.8080\n",
            "Epoch: [75/100]---val, loss: 0.876608, acc: 0.7371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 76/100 [11:23<03:34,  8.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [75/100]---tar, loss: 0.876608, acc: 0.7371\n",
            "\n",
            "Epoch: [76/100]---src, loss: 0.776262, acc: 0.8023\n",
            "Epoch: [76/100]---val, loss: 0.843065, acc: 0.7509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 76/100 [11:32<03:38,  9.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [76/100]---tar, loss: 0.843065, acc: 0.7509\n",
            "Training complete in 11m 32s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: fine-tuning function 호출하여 학습 진행\n",
        "finetune(model, dataloaders, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GR5Y1x4btm4y"
      },
      "outputs": [],
      "source": [
        "def test(model, target_test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    len_target_dataset = len(target_test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            s_output = model.predict(data)\n",
        "            pred = torch.max(s_output, 1)[1]\n",
        "            correct += torch.sum(pred == target)\n",
        "    acc = correct.double() / len(target_test_loader.dataset)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1O2CBxmxtm4y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4461/2129991207.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pkl'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.750943396226415\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model.pkl'))\n",
        "acc_test = test(model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Iuk_Mitm4z"
      },
      "source": [
        "여기까지가 fine-tuning 파트입니다. 실제 학습에서는 learning rate decay 같은 기법도 사용하지만, 이 과제에서는 그것이 핵심이 아니므로 생략합니다.\n",
        "\n",
        "이제 같은 dataloader를 그대로 활용해서 domain adaptation 실험을 이어가봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO4c_QcGtm4z"
      },
      "source": [
        "# Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJwcwLQftm40"
      },
      "source": [
        "Domain adaptation의 핵심 구조는 fine-tuning과 매우 비슷하지만,\n",
        "두 도메인 간 분포 차이를 줄이기 위한 loss function을 추가해야 합니다.\n",
        "\n",
        "여기서는 MMD와 Coral loss를 사용해 두 도메인 간 분포 차이를 계산하는 loss function을 정의합니다. \n",
        "\n",
        "해당 loss를 이용할 수 있도록 새로운 모델 클래스를 정의하고 source의 feature와 label, 그리고 target feature를 모두 이용하도록 학습 스크립트를 수정해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_1xwdJtm40"
      },
      "source": [
        "### Loss function\n",
        "Domain Adaptation에서 가장 많이 사용되는 손실 함수는 MMD (Maximum Mean Discrepancy)입니다.\n",
        "\n",
        "비교를 위해 또 다른 대표적인 손실 함수인 CORAL (CORrelation ALignment)도 함께 살펴봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-wKorUtm40"
      },
      "source": [
        "#### MMD loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MpQH6VFwtm41"
      },
      "outputs": [],
      "source": [
        "class MMD_loss(nn.Module):\n",
        "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n",
        "        super(MMD_loss, self).__init__()\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_mul = kernel_mul\n",
        "        self.fix_sigma = None\n",
        "        self.kernel_type = kernel_type\n",
        "\n",
        "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
        "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
        "        total = torch.cat([source, target], dim=0)\n",
        "        total0 = total.unsqueeze(0).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        total1 = total.unsqueeze(1).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        L2_distance = ((total0-total1)**2).sum(2)\n",
        "        if fix_sigma:\n",
        "            bandwidth = fix_sigma\n",
        "        else:\n",
        "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
        "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
        "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
        "                          for i in range(kernel_num)]\n",
        "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
        "                      for bandwidth_temp in bandwidth_list]\n",
        "        return sum(kernel_val)\n",
        "\n",
        "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
        "        loss = 0.0\n",
        "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
        "        loss = delta.dot(delta.T)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return self.linear_mmd2(source, target)\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            batch_size = int(source.size()[0])\n",
        "            kernels = self.guassian_kernel(\n",
        "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
        "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
        "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
        "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
        "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
        "            loss = torch.mean(XX + YY - XY - YX)\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfUy_2Dtm41"
      },
      "source": [
        "#### CORAL loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uZhKJq15tm41"
      },
      "outputs": [],
      "source": [
        "def CORAL(source, target):\n",
        "    d = source.size(1)\n",
        "    ns, nt = source.size(0), target.size(0)\n",
        "\n",
        "    # source covariance\n",
        "    tmp_s = torch.ones((1, ns)).cuda() @ source\n",
        "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n",
        "\n",
        "    # target covariance\n",
        "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
        "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
        "\n",
        "    # frobenius norm\n",
        "    loss = (cs - ct).pow(2).sum().sqrt()\n",
        "    loss = loss / (4 * d * d)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB2cDp8Gtm41"
      },
      "source": [
        "### Model\n",
        "여기서도 backbone으로는 ResNet-50을 사용합니다.\n",
        "다만 이번에는 ResNet-50의 마지막 classifier layer를 제거한 feature extractor로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UOLx_OSxtm41"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "class ResNet50Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Fc, self).__init__()\n",
        "        model_resnet50 = models.resnet50(pretrained=True)\n",
        "        self.conv1 = model_resnet50.conv1\n",
        "        self.bn1 = model_resnet50.bn1\n",
        "        self.relu = model_resnet50.relu\n",
        "        self.maxpool = model_resnet50.maxpool\n",
        "        self.layer1 = model_resnet50.layer1\n",
        "        self.layer2 = model_resnet50.layer2\n",
        "        self.layer3 = model_resnet50.layer3\n",
        "        self.layer4 = model_resnet50.layer4\n",
        "        self.avgpool = model_resnet50.avgpool\n",
        "        self.__in_features = model_resnet50.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRgdNM3Wtm42"
      },
      "source": [
        "이제 Domain Adaptation을 위한 핵심 모델 클래스를 정의합니다.\n",
        "\n",
        "ResNet-50을 기반으로 하되, bottleneck layer와 새로운 fc layer를 추가합니다.\n",
        "\n",
        "중요한 점은 adapt_loss 함수로 우리가 정의한 MMD 또는 CORAL loss를 forward pass에서 함께 계산한다는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oC5NKJpJtm42"
      },
      "outputs": [],
      "source": [
        "class TransferNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class,\n",
        "                 base_net='resnet50',\n",
        "                 transfer_loss='mmd',\n",
        "                 use_bottleneck=True,\n",
        "                 bottleneck_width=256,\n",
        "                 width=1024):\n",
        "        super(TransferNet, self).__init__()\n",
        "        if base_net == 'resnet50':\n",
        "            resnet = models.resnet50(pretrained=True)\n",
        "            self.base_network = nn.Sequential(*list(resnet.children())[:-1], nn.Flatten())\n",
        "            feature_dim = 2048\n",
        "        else:\n",
        "            # Your own basenet\n",
        "            return None\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.transfer_loss = transfer_loss\n",
        "        ##TODO\n",
        "        bottleneck_list = [\n",
        "                nn.Linear(feature_dim, bottleneck_width),\n",
        "                nn.BatchNorm1d(bottleneck_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.5)\n",
        "            ]\n",
        "        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n",
        "        ##TODO\n",
        "        classifier_layer_list = [\n",
        "            nn.Linear(feature_dim, width),\n",
        "            nn.BatchNorm1d(width),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(width, num_class)\n",
        "        ]\n",
        "        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n",
        "\n",
        "        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n",
        "        self.bottleneck_layer[0].bias.data.fill_(0.1)\n",
        "        for i in range(2):\n",
        "            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n",
        "            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source = self.base_network(source)\n",
        "        target = self.base_network(target)\n",
        "        source_clf = self.classifier_layer(source)\n",
        "        if self.use_bottleneck:\n",
        "            source = self.bottleneck_layer(source)\n",
        "            target = self.bottleneck_layer(target)\n",
        "        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n",
        "        return source_clf, transfer_loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        features = self.base_network(x)\n",
        "        clf = self.classifier_layer(features)\n",
        "        return clf\n",
        "\n",
        "    def adapt_loss(self, X, Y, adapt_loss):\n",
        "        \"\"\"Compute adaptation loss, currently we support mmd and coral\n",
        "\n",
        "        Arguments:\n",
        "            X {tensor} -- source matrix\n",
        "            Y {tensor} -- target matrix\n",
        "            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- adaptation loss tensor\n",
        "        \"\"\"\n",
        "        if adapt_loss == 'mmd':\n",
        "            mmd_loss = MMD_loss()\n",
        "            loss = mmd_loss(X, Y)\n",
        "        elif adapt_loss == 'coral':\n",
        "            loss = CORAL(X, Y)\n",
        "        else:\n",
        "            # Your own loss\n",
        "            loss = 0\n",
        "        return loss\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdPs27btm42"
      },
      "source": [
        "### Train\n",
        "이제 Domain Adaptation 모델을 학습시켜 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OK6P8uMDtm42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/detectron2/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/detectron2/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "transfer_loss = 'mmd'\n",
        "learning_rate = 0.0001\n",
        "transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet50').cuda()\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': transfer_model.base_network.parameters()},\n",
        "    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "lamb = 10 # weight for transfer loss, it is a hyperparameter that needs to be tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpUfcHItm42"
      },
      "source": [
        "학습 함수에서는 source 데이터와 label, target 데이터를 모두 사용해야 하므로, source와 target의 dataloader를 zip으로 묶어서 동시에 iterate합니다.\n",
        "\n",
        "보통 두 도메인의 샘플 수는 다르기 때문에, 여러 epoch에 걸쳐 무작위로 잘 섞이면 전체 데이터를 충분히 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AGlVNI2ktm42"
      },
      "outputs": [],
      "source": [
        "def train(dataloaders, model, optimizer):\n",
        "    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n",
        "    len_source_loader = len(source_loader)\n",
        "    len_target_loader = len(target_train_loader)\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    n_batch = min(len_source_loader, len_target_loader)\n",
        "    for e in range(n_epoch):\n",
        "        stop += 1\n",
        "        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n",
        "        model.train()\n",
        "        for (src, tar) in zip(source_loader, target_train_loader):\n",
        "            data_source, label_source = src\n",
        "            data_target, _ = tar\n",
        "            data_source, label_source = data_source.cuda(), label_source.cuda()\n",
        "            data_target = data_target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            label_source_pred, transfer_loss = model(data_source, data_target)\n",
        "            clf_loss = criterion(label_source_pred, label_source)\n",
        "            loss = clf_loss + lamb * transfer_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n",
        "            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n",
        "            train_loss_total = loss.detach().item() + train_loss_total\n",
        "        acc = test(model, target_test_loader)\n",
        "        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n",
        "        if best_acc < acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'trans_model.pkl')\n",
        "            stop = 0\n",
        "        if stop >= early_stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "nqSCG6-Xtm43",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [ 0/20], cls_loss: 3.2905, transfer_loss: 0.1643, total_Loss: 4.9332, acc: 0.3396\n",
            "Epoch: [ 1/20], cls_loss: 2.6536, transfer_loss: 0.1635, total_Loss: 4.2885, acc: 0.5786\n",
            "Epoch: [ 2/20], cls_loss: 2.0786, transfer_loss: 0.1661, total_Loss: 3.7399, acc: 0.6314\n",
            "Epoch: [ 3/20], cls_loss: 1.7383, transfer_loss: 0.1643, total_Loss: 3.3810, acc: 0.6654\n",
            "Epoch: [ 4/20], cls_loss: 1.4262, transfer_loss: 0.1650, total_Loss: 3.0762, acc: 0.6881\n",
            "Epoch: [ 5/20], cls_loss: 1.3268, transfer_loss: 0.1629, total_Loss: 2.9555, acc: 0.7031\n",
            "Epoch: [ 6/20], cls_loss: 1.2039, transfer_loss: 0.1615, total_Loss: 2.8187, acc: 0.7195\n",
            "Epoch: [ 7/20], cls_loss: 1.0896, transfer_loss: 0.1619, total_Loss: 2.7083, acc: 0.7182\n",
            "Epoch: [ 8/20], cls_loss: 0.9816, transfer_loss: 0.1611, total_Loss: 2.5927, acc: 0.7119\n",
            "Epoch: [ 9/20], cls_loss: 1.0098, transfer_loss: 0.1580, total_Loss: 2.5893, acc: 0.7069\n",
            "Epoch: [10/20], cls_loss: 0.9397, transfer_loss: 0.1513, total_Loss: 2.4523, acc: 0.7434\n",
            "Epoch: [11/20], cls_loss: 0.8944, transfer_loss: 0.1439, total_Loss: 2.3333, acc: 0.7208\n",
            "Epoch: [12/20], cls_loss: 0.8552, transfer_loss: 0.1380, total_Loss: 2.2355, acc: 0.7447\n",
            "Epoch: [13/20], cls_loss: 0.8179, transfer_loss: 0.1270, total_Loss: 2.0882, acc: 0.7472\n",
            "Epoch: [14/20], cls_loss: 0.8387, transfer_loss: 0.1178, total_Loss: 2.0171, acc: 0.7220\n",
            "Epoch: [15/20], cls_loss: 0.7769, transfer_loss: 0.1116, total_Loss: 1.8933, acc: 0.7346\n",
            "Epoch: [16/20], cls_loss: 0.7339, transfer_loss: 0.1051, total_Loss: 1.7852, acc: 0.7396\n",
            "Epoch: [17/20], cls_loss: 0.6860, transfer_loss: 0.1005, total_Loss: 1.6909, acc: 0.7547\n",
            "Epoch: [18/20], cls_loss: 0.6572, transfer_loss: 0.1003, total_Loss: 1.6604, acc: 0.7321\n",
            "Epoch: [19/20], cls_loss: 0.7442, transfer_loss: 0.0981, total_Loss: 1.7254, acc: 0.7258\n"
          ]
        }
      ],
      "source": [
        "# TODO: Domain Adaptation 모델을 학습시키는 함수를 호출하여 학습 진행\n",
        "\n",
        "train(dataloaders, transfer_model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yWvYODRDtm43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_754/827170216.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7547169811320754\n"
          ]
        }
      ],
      "source": [
        "transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n",
        "acc_test = test(transfer_model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_transfer_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "detectron2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
