{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 블록 중 최대 토큰 개수 (max token length): 226\n",
      "전체 블록 중 최대 생성토큰 개수 (gen_length): 182\n",
      "전체 블록 중 최대 토큰 결과 (max_sentence): 1) If you get smart, then you can do better in school. 2) If you are bored, then you may go to school for a break. Therefore, the answer is not (B). 3) Cold and flu can be spread by people coming into contact with each other, like in school. 4) Tests are not really good reasons for attending school. 5) You spend time in school, but school is also a place to learn and study. 6) The answer should be to learn something, so we can eliminate option (E). 7) Spending time is also not a reason for attending school, so we can eliminate option (E). 8) The answer should be that it is good to learn something, and school is a place to learn things. 9) Since the other answers do not match, the answer is (A) get smart.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# GPT-2 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "def extract_answer_token_length(text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에서 \"A:\" 이후의 텍스트를 추출하고,\n",
    "    해당 부분의 토큰 길이를 반환합니다.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"A:\\s*(.*)\", text, re.DOTALL)\n",
    "    if match:\n",
    "        answer_text = match.group(1).strip()\n",
    "        tokens = tokenizer.tokenize(answer_text)\n",
    "        token_count = len(tokens)\n",
    "        return answer_text, token_count\n",
    "    else:\n",
    "        return None, 0\n",
    "\n",
    "# 텍스트 파일 읽기 (예시 파일명: data.txt)\n",
    "with open(\"cqa/cqa_vanilla_10/cqa_vanilla_10_0/correct_data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 각 데이터 블록은 \"<|end_of_text|>\" 로 구분되어 있다고 가정\n",
    "blocks = content.split(\"<|end_of_text|>\")\n",
    "\n",
    "max_token_length = 0  # 전체 블록 중 최대 토큰 개수를 저장할 변수\n",
    "max_sentence=''\n",
    "gen_length=0\n",
    "\n",
    "for i, block in enumerate(blocks):\n",
    "    # \"A:\" 이후 텍스트와 토큰 수 계산\n",
    "    answer_text, answer_token_count = extract_answer_token_length(block)\n",
    "    # 블록 전체의 토큰 수 계산\n",
    "    block_tokens = tokenizer.tokenize(block)\n",
    "    block_token_count = len(block_tokens)    \n",
    "    \n",
    "    # 최대 토큰 개수 갱신\n",
    "    if block_token_count > max_token_length:\n",
    "        max_token_length = block_token_count\n",
    "        gen_length=answer_token_count\n",
    "        max_sentence=answer_text\n",
    "\n",
    "print(\"전체 블록 중 최대 토큰 개수 (max token length):\", max_token_length)\n",
    "print(\"전체 블록 중 최대 생성토큰 개수 (gen_length):\", gen_length)\n",
    "print(\"전체 블록 중 최대 토큰 결과 (max_sentence):\", max_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
